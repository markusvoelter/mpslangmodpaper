
\section{Related Work}
\label{Related} 

This paper addresses language modularity with MPS, a topic that concerns many
different aspects. In this section we discuss related work focussing on
projectional editing, as well as language modularity in terms of syntax, type
systems, semantics and IDE.



\subsection{Modular Parsers}

\todo{First step: Parsing}

Kats, Visser and Wachsmut in \cite{KatsVW10} describe nicely the problems with
non-declarative grammar specifications and the resulting problems for
composition of independently developed grammars. The biggest problem with
grammar formalisms that cover only subsets of the class of context-free
grammars is that these are not closed under composition: resulting grammars are
likely to fall out of the respective grammar class. Composition (without
invasive change) is prohibited. Grammar formalisms that implement the full set
of context-free grammars do not have this problem and support composition much
better. Schwerdtfeger and van Wyk discuss discuss the issues surrounding grammar
composition as well; they also describe a way of verifying early (i.e. before
the actual composition attempt) if two grammars are composable or not
\cite{SchwerdfegerW09}.

For example, the Syntax Definition Formalism (SDF, \cite{HeeringHKR89}) is a
scannerless GLR parser. Since it parses tokens and characters in a context-aware
fashion, there will no ambiguities if grammars are composed that both define the
same token or production \emph{in different contexts}. This allows, for example,
to embed SQL into Java (as Bravenboer et al. discuss in \cite{BravenboerDV07}).
However, if the same syntactic form is used by the composed grammars \emph{in
the same location}, then some kind of disambiguation is necessary. Such
disambiguations are typically called quotations and antiquotations and are
defined in a third the grammar that describes the composition of two other
independent grammars (discussed in \cite{BravenboerV04}). The SILVER/COPPER
system described by van Wyk in \cite{WykBGK08} solves the ambiguities via
disambiguation functions written specifically for each combination of ambiguosly
composed grammars. Note that in MPS such disambiguation is never necessary. We
discuss the potential for ambiguity and the way solves the problem in
\todo{ref}.  


Given a set of extensions for a language, SILVER/COPPER allows users to include
a subset of these extensions into a program as needed (this has been implemented
for Java (AbleJ \cite{WykKBS07}) and and for the SPIN's Promela language (AbleP
\cite{MaliW11}). A similar approach is discussed for an SDF-based syste, in
\cite{BravenboerV07}. However, this only works as long as the set of included
extensions (which have presumably been developed independent from each other)
are not ambiguous with each other. In case of ambiguities, disambiguations have
to be defined as described above.

Polyglot, an extensible compiler framework for Java \cite{NystromCM03} also
uses an extensible formalism and parser to supports adding, modifying or
removing productions and symbols defined in a base grammar. However, since
Polyglot uses LALR grammars, users must make sure \emph{manually} that the base
language and the extension stays in the LALR subclass. 

 
 


\section{Modular Compilers}

Modular compilers make use of modular parsers and add modular specification of
semantics, including static semantics (constraints and type systems) as well as
execution semantics.

Most systems describe static semantics using attribute grammars. Attribute
grammars associate attributes with AST elements. These attributes can capture
arbitrary data about the element (such as its type). Attributes of one element
can be computed from attributes of related elements (such as children). Example
of systems that make use of attribute grammars for type computation and type
checking include SILVER (mentioned above) JastAdd \cite{HedinM03} and LISA
(discussed in more detail in the next section). Forwarding (introduced in
\cite{WykMBK02}) is a mechanism that improves the modularity of attributed
grammars by delegating the lookup of an attribute value to another element. 
MPS' type system is different from attribute grammars. Attributes values are
calculated (recursively) from attributes of other, references nodes. MPS' type
system rules are declarative: users specify typing rules for language concepts
and MPS then ``instantiates'' each rule for each AST node. A solver then solves
all type equations in that AST. This way, the typing rules of elements
contributed by languge extensions can \emph{implicitly} affect the overall
typing of the program. 


The execution semantics of MPS is defined via translation down to 



\subsection{Projectional Editing}

Projectional editing (also known as structural editing) is not a new idea. An
early example is the Incremental Programming Environment (IPE,
\cite{Medina-MoraF81}). It uses a structure editor for users to interact wit the
program and then incrementally compiles and executes the resulting program tree.
It supports the definition of several notations for the same program as well as
partial projections. However, the projectional editor forces users to build the
program tree top-down. For examplec, to enter \ic{2 + 3} users first have to
enter the \ic{+} and then fill in the two arguments. This is very tedious and
forces users to understand the program structure. MPS in contrast goes a long
way in supporting editing gestures that much more resemble text editing. The IPE
also does not address language modularity. In fact it comes with a fixed, C-like
language. The IPE does not come with a facility to easily define new languages.
It is not bootstrapped. Another projectional system is GANDALF \cite{Notkin85}.
Its ALOEGEN component generates projecitonal editors from a language
specification. It has the same usability problems as described for IPE. This is
nicely expressed in \cite{NPS}: \emph{Program editing time will be considerably
slower than normal keyboard entry although actual time spent programming
non-trivial programs should be reduced due to reduced error rates.}.

The synthesizer generator described in \cite{RepsT84} also supports structural
editing (also referred as template-based editing). However, at the fine-grained
expression level, textual input and parsing is used. This removes many of the
advantages of projectional editing in the first place, because simple language
composition \emph{even at the expression level} is prohibited. MPS does not use
this ``trick'', and instead supports projectional editing also on expression
level. 

Bagert and Friesen describe a multi-language syntax directed editor in
\cite{BagertF87}. However, this tool supports only referencing, syntactic
composition is not supported. 

In terms of contemporary projectional editors, The Intentional Domain Workbench
(IDW) \cite{SimonyiCC06} is another projectional editor that has been used in
real projects. An impressive presentation about its capabilities can be found in
an InfoQ presentation titled "Domain Expert DSL" (\ic{http://bit.ly/10BsWa}).
IDW is conceptually very similar to MPS, although quite different in many
details.




















\label{cedalion} 
Cedalion \cite{cedalion} is a host language for defining internal DSLs. It uses
a projectional editor and semantics based on logic programming. Both Cedalion and
language workbenches such as MPS aim at combining the best of both worlds from
internal DSLs (combination and extension of languages, integration with a host
language) and external DSLs (static validation, IDE support, flexible syntax).
Cedalion starts out from internal DSLs and adds static validation and
projectional editing, the latter avoiding ambiguities resulting from combined
syntaxes. Language workbenches start from external DSLs and add modularization,
and, as a consequence of implementing base languages with the same tool,
optional tight integration with general purpose host languages. We could not
have used Cedalion as the platform for mbeddr tough, since we implemented our
own base language (C), and the logic-based semantics would not have been a good
fit.










LISA \cite{MernikLAZ02} generates a structural editor (as well as other
language tools) from a language specification. 










\subsection{Parser-Based}





Eclipse Xtext (\ic{http://eclipse.org/Xtext}) supports the creation of extremely
powerful text editors (with code completion, error checking and syntax coloring)
from an enhanced EBNF-like grammar definition. It also generates a meta-model
that represents the abstract syntax of the grammar as well as a parser that
parses sentences of the language and builds an instance of the meta-model. Since
it uses the Eclipse Modeling Framework (EMF) as the basis for its meta models,
it can be used together with any EMF-based model transformation and code
generation tool (examples include Xpand, ATL, and Acceleo, all at
\ic{http://eclipse.org/modeling}). Language referencing is easily possible: code
completion for references into other models as well as cross-model and
cross-language consistency checks in the editor are supported natively. Just
like with any other reference, scopes may have to be defined manually. Language
reuse, extension and embedding are quite limited, though. It is possible to make
a language extend \emph{one} other language. Concepts from the base language can
be used in the sub language and it is possible to redefine grammar rules defined
in the base language. Creating new subtypes (in terms of the meta-model) of
language elements in the base language is also possible. However, it is not
possible to embed arbitrary languages or language modules. This is mainly
because the underlying parser technology is antlr \todo{ref to paper!} which is
a classical two phase LL(*) parser which has problems with grammar composition
\cite{BravenboerV08}. 

Monticore (\ic{http://monticore.org}) is another parser-based language
engineering environment that generates parsers, meta-models, and editors based
on extended grammar. Currently, the group at RWTH Aachen university works on
modularizing languages \cite{KrahnRV10}. Languages can extend each other and can
be embedded within each other. An important idea is the ability to not
regenerate the parsers or any of the related tools after a combined language has
been defined.

A particularly interesting comparison can be made with the Helvetia system by
Renggli et al. \cite{2010_renggli_embedding_languages_without_breaking_tools}.
It supports language embedding and extension of Smalltalk using
\emph{homogeneous} extension, which means that the host language (Smalltalk) is
also used for \emph{defining} the extensions (in contrast to some of the
embedded DSLs discussed above, Helvetia can work with custom grammars for the
DSLs). The authors argue that the approach is independent of the host language
and could be used with other host languages as well. While this is true in
principle, the implementation strategy heavily relies on some aspects of the
Smalltalk system that are not present for other languages, and in particular,
not in C. Also, since extensions are defined in the host language, the complete
implementation would have to be redone if the approach were used with another
language. This is particularly true for IDE support, where the Smalltalk IDE is
extended using this IDE's APIs. mbeddr uses a \emph{heterogeneous} approach
which does not have these limitations: MPS provides a language-agnostic
framework for language and IDE extension that can be used with any language,
once the language is implemented in MPS.
  
In the same paper, Renggli and his colleagues introduce three different flavors
of language extension. A \emph{pidgin} creatively bends the existing syntax of
the host language to to extend its semantics. A \emph{creole} introduces
completely new syntax and custom transformations back to the host language. An
\emph{argot} reinterprets the semantics of valid host language code. In
terms of this classification, bith extension and embedding are creoles. 


Several works avoid these limitations by making language definition and
extension first class. Early examples include the Synthesizer
Generator~\cite{RepsT84} as well as the Meta Environment~\cite{Klint93}. Both
generate editors and other IDE aspects from a language definition. The topic is
still actively researched. For example, Bravenboer et al.
\cite{2004_bravenboer_concrete_syntax_for_objects} and Dinkelacker
\cite{2011_dinkelaker_incremental_concrete_syntax_for_embedded_languages} 
provide custom concrete syntax, Bracha \cite{2004_bracha_pluggable_type_systems}
provides pluggable type systems and Erweg et al.
\cite{2011_erdweg_growing_a_language_environment} discuss modular IDE
extensions. Eisenberg and Kiczales propose explicit
programming~\cite{EisenbergK07} which supports semantic extension as well as
editing extensions (concrete syntax) for a given base language. 

Our approach is similar in that we provide extensions of syntax, type systems,
semantics and IDE support for a base language. mbeddr is different in that it
extends C, in that we use a projectional editor and in that we address IDE
extension including advanced features such as type systems, refactorings and the
debugger. The use of a projectional editor is especially significant, since this
enables the use of non-textual notations and annotation of cross-cutting meta
data.



Note that while parser-based approach are becoming more flexible (as illustrated
by some of the tools mentioned in this section), they will not be able to
work with non-parseable code, inlined tables, diagrams or annotations. 

For a general overview of language workbenches, please refer to the Language
Workbench Competition at \ic{http://languageworkbenches.net}. Participating
tools have to implement a standardized language and document the implementation
strategy. This serves as a good tutorial of the tool and makes them comparable. 
As of June 2012, the site contains 15 submissions.


\subsection{Other}



FURCAS (\ic{http://www.furcas.org/}) is a tool that is developed by SAP and FZI
Karlsruhe. FURCAS stores models in an abstract structure. However, for editing
it "projects" the model into plain ASCII. So when editing the model, users
actually edit ASCII text. Consequently, syntax definition also includes a
definition of indentation and white space conventions, otherwise the projection
could not work. Second, a lot of effort has to be put into retaining object
identity \cite{Goldschmidt08}. If an abstract structure is projected into text,
and then something is moved around and saved back into the abstract structure,
it has to be made sure the objects (identified by UUIDs) are not deleted and
recreated, but really just moved. This is important to make sure that references
between models which are based on the UUIDs and not (qualified) names remain
valid. By using scannerless parsers it is possible to combine different
languages, however a combined grammar has to be defined and the parser has to be
regenerated to be able to use the composed language. As a consequence of the
projectional approach, it is possible to define several syntaxes for the same
abstract structure or define views and subsets for a model. FURCAS also
generates IDE-like editors (based on Eclipse).


\subsection{Languages + IDE}

\todo{For all of those tools discuss syntax, type sys and semantics. And maybe
remove one or two of them since I'll add all the other related work below? }

\todo{comapre to parser-based stuff in general - references: spoofax stuff: you
need espace syntax to embed languages, and you have to define the ``composed
lang'' explicitly.}


 



SDF \cite{HeeringHKR89} (\ic{http://strategoxt.org/Sdf}), developed by the
University of Amsterdam, uses scannerless parsers. Consequently, languages can
be embedded within each other. However syntactic escapes (quotations and
antiquotations) have to be defined in the adapter language. This is not
necessary in MPS, which leads to a smoother integration among languages. 
Several IDEs have been developed for SDF and the languages defined with it. The
first one was the Meta Environment \todoref{}. More recent ones include Rascal
\todoref{} and Spoofax \cite{KatsV10}. The latter two both provide Eclipse-based
IDE support for languages defined via SDF. In both cases the IDE support for the
composed languages is still limited (for example, at the time of this writing,
Spoofax only provides syntax highlighting for an embedded language, but no code
completion), but will be coming. Spoofax uses the Stratego \todoref{} term
rewriting engine for expressing transformations. Spoofax also uses term
rewriting for expressing typing rules (a rule \ic{type-of} rewrites an AST term
to its type term), so type systems are modular and extensible as well.
\todo{More details on Rascal's language composition?}
\todo{More details on MetaEnv?}

\todo{cite some more of eelco's work? Like the SQL-in-Java thing?}

















\subsection{Language Only}


The contribution of this paper is the systematic definition and classification
of language modularization approaches, as well as the challenges and solution
involved regarding syntax definition, type systems, transformations and IDE
support. The idea of language modularization and composition itself is not new,
however. 

We already discussed the language modularization and composition approaches
proposed by Mernik et. al. \cite{MernikHS05} in \sect{classification}. 

 
\phead{Incremental Extension of Languages} was first popularized in
the context of Lisp, where definition of language extensions to solve problems
in a given domain is a well-known approach. Guy Steele's Growing a Language
keynote explains the idea well \cite{Steele99}. The paper on Xoc
\cite{CoxBCKK08} describes the idea of extending C incrementally, albeit without
extending a corresponding IDE. Sergey Dmitriev discusses the idea of language
and IDE extension in his article on Language Oriented Programming
\cite{lopnextprogrammingparadigm}, which uses MPS as the tool to achieve the
goal.

\phead{Macro Systems} support the definition of additional syntax for
existing languages. Macro expansion maps the new syntax to valid code in the
extended language, and this mapping is expressed with host language code instead
of a separate transformation language.They differ with regard to degree of
freedom they provide for the extension syntax, and whether they support
extensions of type systems and IDEs. The most primitive macro system is the C
preprocessor which performs pure text replacement during macro expansion. The
Lisp macro system is more powerful because it is aware of the syntactic
structure of Lisp code. An example of a macro system with limited syntactic
freedom is the The Java Syntactic Extender \cite{504285} where all macros have
to begin with names, and a limited set of syntactic shapes is supported. In
OpenJava \cite{TatsuboriCIK99}, the locations where macros can be added is
limited. More fine-grained extensions, such as adding a new operator, are not
possible.

% Our work relates to macro systems such as Open Java~\cite{TatsuboriCIK99}in that
% mbeddr customizes the translation of language extensions.
% However, mbeddr uses non-local transformations as well; those are not easily
% expressible with macros. Also, traditionally, macros have not addressed IDE
% extension.


\phead{Language Cascading} refers to a form of language combination where a
program expressed in languge $l_1$ is translated into a program expressed in
language $l_2$. Essentially this is what every code generator or compiler does;
the languages themselves are not related in any way except through the
transformation engine, which is why we don't consider this as an example of
language modularization and composition. An example of this approach is KHEPERA
\cite{FaithNP97}

\phead{Full Language Extension and Composition} refers to the case where
arbitrary syntax can be added to a host language, and type systems and IDEs are
aware of the extension. This paper classifies four approaches for doing this.
Existing implementations exist. For example, Bravenboer and Visser describe how
SQL can be embedded into Java to prevent SQL injection attacks
\cite{BravenboerDV07}. 

A more recent publication
\cite{Erdweg-OOPSLA-2011} also based on SDF introduces SugarJ, which supports
library based languages extension. \cite{Erdweg-GPCE-2011} adds IDE support.

Open compilers such as Jastadd~\cite{EkmanH07} are related in that 
they support language extension and custom transformation. However, while
open compilers can typically be extended with independent modules, the input
language often requires invasive adaptation. Also, open compilers do not address
IDE extension. 

\phead{Internal DSLs} are languages embedded in general purpose host
languages. Suitable host languages are those that provide a flexible syntax, as
well as meta programming facilities to support the definition of new
abstractions with a custom concrete syntax. For example \cite{HoferORM08}
describes embedding DSLs in Scala. In this paper we don't address internal DSLs,
because IDE support for the embedded languages is not available in these cases,
and we consider IDE support for the composed languages essential.
The landmark work of Hudak~\cite{1998_hudak_modular_dsl_and_tools} introduces
embedded DSLs as language extensions of Haskell. While Haskell provides advanced
concepts that enable such extensions, the new DSLs are essentially just
libraries built with the host language and are not first class language
entities: they do not define their own syntax, compiler errors are expressed in
terms of the host language, no custom semantic analyses are supported and no
specific IDE-support is provided. Essentially all internal DSLs expressed with
dynamic languages such as Ruby or Groovy, but also those embedded
in static languages such as Scala suffer from these limitations. 


\todo{More work on Semantic Extensibility}

\todo{(e.g., AbleJ, LISA, Modular denotational semantics, 
  Polyglot, Tattoo, ...) }
