
\section{Related Work}
\label{Related} 

This paper addresses language modularity with MPS, a topic that concerns many
different aspects. In this section we discuss related work focusing on
modular parsers, projectional editing, modular compilers, projectional editing
and modular IDEs. We conclude with a section on related work that does not fit
these four categories.


\subsection{Modular Parsers}

As we have seen in this paper, modular composition of concrete syntax is the
basis for many interesting approaches to language composition. Hence we start by
discussing modularization and composition of grammars.

Kats, Visser and Wachsmut in~\cite{KatsVW10} describe nicely the problems with
non-declarative grammar specifications and the resulting problems for
composition of independently developed grammars. The biggest problem with
grammar formalisms that cover only subsets of the class of context-free
grammars is that these are not closed under composition: resulting grammars are
likely to fall out of the respective grammar class. Composition (without
invasive change) is prohibited. Grammar formalisms that implement the full set
of context-free grammars do not have this problem and support composition much
better. Schwerdtfeger and van Wyk discuss discuss the issues surrounding grammar
composition as well; they also describe a way of verifying early (i.e. before
the actual composition attempt) if two grammars are composable or not
\cite{SchwerdfegerW09}.

For example, the Syntax Definition Formalism (SDF,~\cite{HeeringHKR89}) is a
scannerless GLR parser. Since it parses tokens and characters in a context-aware
fashion, there will no ambiguities if grammars are composed that both define the
same token or production \emph{in different contexts}. This allows, for example,
to embed SQL into Java (as Bravenboer et al. discuss in~\cite{BravenboerDV07}).
However, if the same syntactic form is used by the composed grammars \emph{in
the same location}, then some kind of disambiguation is necessary. Such
disambiguations are typically called quotations and antiquotations and are
defined in a third the grammar that describes the composition of two other
independent grammars (discussed in~\cite{BravenboerV04}). The SILVER/COPPER
system described by van Wyk in~\cite{WykBGK08} solves the ambiguities via
disambiguation functions written specifically for each combination of ambiguosly
composed grammars. Note that in MPS such disambiguation is never necessary. We
discuss the potential for ambiguity and the way solves the problem at the end of
\sect{embedding}.  


Given a set of extensions for a language, SILVER/COPPER allows users to include
a subset of these extensions into a program as needed (this has been implemented
for Java (AbleJ~\cite{WykKBS07}) and and for the SPIN's Promela language (AbleP
\cite{MaliW11}). A similar approach is discussed for an SDF-based syste, in
\cite{BravenboerV07}. However, this only works as long as the set of included
extensions (which have presumably been developed independent from each other)
are not ambiguous with each other. In case of ambiguities, disambiguations have
to be defined as described above.

Polyglot, an extensible compiler framework for Java~\cite{NystromCM03} also
uses an extensible formalism and parser to supports adding, modifying or
removing productions and symbols defined in a base grammar. However, since
Polyglot uses LALR grammars, users must make sure \emph{manually} that the base
language and the extension stays in the LALR subclass. 

 
\subsection{Projectional Editing}

Projectional editing is an alternative approach for handling the relationship
between the concrete syntax and the abstract syntax, i.e. it is an alternative
to parsing. As we have seen, it simplifies modularization and composition.

Projectional editing (also known as structural editing) is not a new idea. An
early example is the Incremental Programming Environment (IPE,
\cite{Medina-MoraF81}). It uses a structure editor for users to interact wit the
program and then incrementally compiles and executes the resulting program tree.
It supports the definition of several notations for the same program as well as
partial projections. However, the projectional editor forces users to build the
program tree top-down. For examplec, to enter \ic{2 + 3} users first have to
enter the \ic{+} and then fill in the two arguments. This is very tedious and
forces users to understand the program structure. MPS in contrast goes a long
way in supporting editing gestures that much more resemble text editing. The IPE
also does not address language modularity. In fact it comes with a fixed, C-like
language. The IPE does not come with a facility to easily define new languages.
It is not bootstrapped. Another projectional system is GANDALF~\cite{Notkin85}.
Its ALOEGEN component generates projecitonal editors from a language
specification. It has the same usability problems as described for IPE. This is
nicely expressed in~\cite{NPS}: \emph{Program editing time will be considerably
slower than normal keyboard entry although actual time spent programming
non-trivial programs should be reduced due to reduced error rates.}.

The synthesizer generator described in~\cite{RepsT84} also supports structural
editing (also referred as template-based editing). However, at the fine-grained
expression level, textual input and parsing is used. This removes many of the
advantages of projectional editing in the first place, because simple language
composition \emph{even at the expression level} is prohibited. MPS does not use
this "trick", and instead supports projectional editing also on expression
level. 

Bagert and Friesen describe a multi-language syntax directed editor in
\cite{BagertF87}. However, this tool supports only referencing, syntactic
composition is not supported. 

In terms of contemporary projectional editors, The Intentional Domain Workbench
(IDW)~\cite{SimonyiCC06} is another projectional editor that has been used in
real projects. An impressive presentation about its capabilities can be found in
an InfoQ
presentation\footnote{http://www.infoq.com/presentations/DSL-Magnus-Christerson-Henk-Kolk} titled "Domain Expert DSL".
IDW is conceptually very similar to MPS, although quite different in many
details.
 


\subsection{Modular Compilers}

Modular compilers make use of modular parsers and add modular specification of
semantics, including static semantics (constraints and type systems) as well as
execution semantics.

Most systems describe static semantics using attribute grammars. Attribute
grammars associate attributes with AST elements. These attributes can capture
arbitrary data about the element (such as its type). Attributes of one element
can be computed from attributes of related elements (such as children). Example
of systems that make use of attribute grammars for type computation and type
checking include SILVER (mentioned above) JastAdd~\cite{HedinM03} and LISA
(discussed in more detail in the next section). Forwarding (introduced in
\cite{WykMBK02}) is a mechanism that improves the modularity of attributed
grammars by delegating the lookup of an attribute value to another element. 
MPS' type system is different from attribute grammars. Attributes values are
calculated (recursively) from attributes of other, references nodes. MPS' type
system rules are declarative: users specify typing rules for language concepts
and MPS then "instantiates" each rule for each AST node. A solver then solves
all type equations in that AST. This way, the typing rules of elements
contributed by language extensions can \emph{implicitly} affect the overall
typing of the program. 


For language extension, the execution semantics is defined in MPS via
transformation down to the base language. In~\cite{WykBGK08}, van Wyk discusses
under which circumstances such transformations are valid. In particular, the
changes to the overall AST must be local. No global changes to the AST are
allowed. This is to avoid unintended interactions between several independently
developed extensions used in the same program. In MPS such purely local changes
are called reduction rules. In our experience, it is also feasible to add
additional elements to the AST \emph{in select places} based on the use of an
extension. In MPS, this is achieved using weaving rules. However, in both cases
(local reduction and selective adding) there is no way to detect in advance
whether using two extensions in the same program will lead to conflicts.

More formal ways to define semantics include denotational semantics, operational
semantics and and a mapping to a formally defined action language. These have
been modularized to make them composable. For example, Mosses describes modular
structural operational semantics~\cite{Mosses-JLAP-2004} and language
composition by combining action semantics modules~\cite{DohM03}. 

Aspect orientation supports the modularization of cross-cutting concerns. This
has also been applied to language development. For example, Rebernak el al.
\cite{RebernakMWG09} discuss AspectLisa and AspectG. AspectLisa supports adding
new, cross-cutting attribute grammar attributes into a Lisa language definition.
AspectG allows weaving additional action code into ANTLR grammars. Note that
both AspectLisa and AspectG address semantics and not the grammar itself. They
do not support aspect-oriented extension of the concrete syntax. 

% ^\s\stitle = \{(.*)\}
%   title = \{\{$1\}\}
%   
% ^\s\sbooktitle = \{(.*)\}
%   booktitle = \{\{$1\}\}
%   
% ^\tTitle = \{(.*)\}
%     Title = \{\{$1\}\}
%     
% ^\tBooktitle = \{(.*)\}
%     Booktitle = \{\{$1\}\}    

 
\subsection{Modular IDEs}

Now that we have discussed many of the fundamentals that enable modular
languages, we can take at tools that, from a language definition, create a
language aware-editor and other IDE aspects. 


Among the early examples are the Synthesizer Generator~\cite{RepsT84}, mentioned
above, as well as the Meta Environment~\cite{Klint93}. The provides an editor
for languages defined via and ASF+SDF, i.e. it is parser-based. More recent
tools in the ASF+SDF family include Rascal~\cite{KlintSV09} and Spoofax
\cite{KatsV10}. Both provide Eclipse-based IDE support for languages defined via
SDF. In both cases the IDE support for the composed languages is still limited
(for example, at the time of this writing, Spoofax only provides syntax
highlighting for an embedded language, but no code completion), but will be
improved. For implementing semantics, Rascal uses a Java-like language that has
been extended with features for program construction, transformation and
analyses. Spoofax uses term rewriting based on the Stratego
\cite{BravenboerKVV08} language. An interesting tool is SugarJ
\cite{Erdweg-OOPSLA-2011} also based on SDF introduces SugarJ, which supports
library based languages extension.~\cite{Erdweg-GPCE-2011} adds Spoofax-based
IDE support.


SmartTools~\cite{AttaliCDFPP01} supports generating editors for XML schemas. Based on
assigning UI components to AS elements, it can project an editor for programs.
However, this projectional editor does not try to emulate text-like editing as
MPS does, so there is no convenient way for editing expressions. To do this, a
grammar-based concrete syntax can be associated with a the AS elements defined
in the schema. Based on this definition, SmartTools then provides a text-based
representation for the language. However, this prevents syntax composition (as
in used in extension and embedding). SmartTools only supports homogeneous files.
Different UI components and grammars can be defined for the same AS, supporting
multi-notation editing. Static semantics is implemented based on the Visitor
pattern~\cite{GOF95}. SmartTools provides support for much of the infrastructure
and makes using Visitors simple. For transformation, SmartTools provides Xpp, a
transformation language that provides a shorter syntax for XSLT-based XML
transformations.

LISA~\cite{MernikLAZ02} (mentioned earlier) supports the definition of language
syntax (in a BNF-like way) and semantics (via attribute grammars) in one
integrated specification language. It then then derives, among other things, a
syntax-aware text editor for the language, as well as various graphical and
structural viewing and editing facilities. Users can use inheritance and
aspect-orientation to define sub-grammars. The use of this approach for
incremental language development is detailed in~\cite{MernikZ05}. However, users
have to make sure manually that those sub-grammars remain unambiguous.
Combination of independently developed grammars (or sub-grammars) is not
supported. LISA supports interactive debugging and program state visualization
based on interpreting programs based on the semantic parts of the language
specification.

\todo{check all references for completeness and formatting}

Eclipse Xtext (\footnote{http://eclipse.org/Xtext}) generates sophisticated
parser-based editors from an EBNF-like language specification. Syntactic
composition is limited since Xtext is based on antlr~\cite{ParrQ95} which
is a classical two phase LL(*) parser. It is possible to make a language extend
\emph{one} other language. Concepts from the base language can be used in the
sub language and it is possible to redefine grammar rules defined in the base
language. Combination of independently defined extensions or embedding is not
supported. Xtext's abstract syntax is based on EMF Ecore
\footnote{http://eclipse.org/emf}, it can be used together with any EMF-based
model transformation and code generation tool (examples include Xpand, ATL, and
Acceleo, all located at the Eclipse Modeling
site\footnote{http://eclipse.org/modeling}.
Static semantics is based on constraints written in Java or on frameworks that support declarative
description of type systems such as
XTS\footnote{http://code.google.com/a/eclipselabs.org/p/xtext-typesystem/}.

Monticore (\footnote{http://monticore.org}) is another parser-based language
engineering environment that generates parsers, metamodels, and editors based
on extended grammar. Currently, the group at RWTH Aachen university works on
modularizing languages~\cite{KrahnRV10}. Languages can extend each other and can
be embedded within each other. An important idea is the ability to not
regenerate the parsers or any of the related tools after a combined language has
been defined. However, ambiguities have to be avoided manually.



The Helvetia system by
\cite{2010_renggli_embedding_languages_without_breaking_tools} supports language
embedding and extension of Smalltalk using \emph{homogeneous} extension, which
means that the host language (Smalltalk) is also used for \emph{defining} the
extensions. The authors argue that the approach is independent of the host
language and could be used with other host languages as well. While this is true
in principle, the implementation strategy heavily relies on some aspects of the
Smalltalk system that are not present for other languages. Also, since
extensions are defined in the host language, the complete implementation would
have to be redone if the approach were used with another language. This is
particularly true for IDE support, where the Smalltalk IDE is extended using
this IDE's APIs. MPS uses a \emph{heterogeneous} approach which does not have
these limitations: MPS provides a language-agnostic framework for language and
IDE extension that can be used with any language, once the language is
implemented in MPS.
  


\label{cedalion} 
Cedalion~\cite{cedalion} is a host language for defining internal DSLs. It uses
a projectional editor and semantics based on logic programming. Both Cedalion
and MPS aim at combining the best of both worlds from internal DSLs (combination
and extension of languages, integration with a host language) and external DSLs
(static validation, IDE support, flexible syntax).
Cedalion starts out from internal DSLs and adds static validation and
projectional editing, the latter avoiding ambiguities resulting from combined
syntaxes. MPS starts from external DSLs and add modularization, and, as a
consequence of implementing base languages with the same tool, optional tight
integration with general purpose host languages. 



For a general overview of language workbenches, please refer to the Language
Workbench Competition\footnote{http://languageworkbenches.net}. Participating
tools have to implement a standardized language and document the implementation
strategy. This serves as a good tutorial of the tool and makes them comparable. 
As of June 2012, the site contains 15 submissions.


\subsection{Other Related Work}

We already discussed the language modularization and composition approaches
proposed by Mernik et. al.~\cite{MernikHS05} in \sect{classification}. In the
Helvetia paper~\cite{2010_renggli_embedding_languages_without_breaking_tools} 
Renggli and his colleagues introduce three different flavors
of language extension. A \emph{pidgin} creatively bends the existing syntax of
the host language to to extend its semantics. A \emph{creole} introduces
completely new syntax and custom transformations back to the host language. An
\emph{argot} reinterprets the semantics of valid host language code. In
terms of this classification, both extension and embedding are creoles. 


The idea of incremental extension of languages  was first popularized in the
context of Lisp, where definition of language extensions to solve problems in a
given domain is a well-known approach. Guy Steele's Growing a Language keynote
explains the idea well~\cite{Steele99}. Sergey Dmitriev discusses the idea of
language and IDE extension in his article on Language Oriented Programming
\cite{lopnextprogrammingparadigm}, which uses MPS as the tool to achieve the
goal.

Macro Systems support the definition of additional syntax for
existing languages. Macro expansion maps the new syntax to valid code in the
extended language, and this mapping is expressed with host language code instead
of a separate transformation language.They differ with regard to degree of
freedom they provide for the extension syntax, and whether they support
extensions of type systems and IDEs. The most primitive macro system is the C
preprocessor which performs pure text replacement during macro expansion. The
Lisp macro system is more powerful because it is aware of the syntactic
structure of Lisp code. An example of a macro system with limited syntactic
freedom is the The Java Syntactic Extender~\cite{504285} where all macros have
to begin with names, and a limited set of syntactic shapes is supported. In
OpenJava~\cite{TatsuboriCIK99}, the locations where macros can be added is
limited. More fine-grained extensions, such as adding a new operator, are not
possible.

Language Cascading refers to a form of language combination where a
program expressed in language $l_1$ is translated into a program expressed in
language $l_2$. Essentially this is what every code generator or compiler does;
the languages themselves are not related in any way except through the
transformation engine, which is why we don't consider this as an example of
language modularization and composition. An example of this approach is KHEPERA
\cite{FaithNP97}. Cascading is referred to as Piggybacking
and Pipelining in Mernik et al.'s classification~\cite{MernikHS05}.


A particular advantage of projectional editing is that it can combine several
notational styles in one fragment; examples include text, tables, symbols
(fraction bars, square roots or big sums). All of these notations are seamlessly
integrated in one program and can be \emph{defined with the same formalism} (as
mentioned earlier, MPS supports text, tables and syntax. Graphics will supported
in 2013). Other approaches for integrating different notational styles exist.
For example,~\cite{EngelenB10-0} integrates textual and graphical notations
based on grammarware and Eclipse modeling technologies. However, such an
approach requires deadling with \emph{separate} tools for the graphical and the
textual aspects, leading to a high degree of accidental complexity in the
resulting implementation and mismatches in the resulting tool, as the author
knows from personal experience.

Internal DSLs are languages embedded in general purpose host
languages. Suitable host languages are those that provide a flexible syntax, as
well as meta programming facilities to support the definition of new
abstractions with a custom concrete syntax. For example~\cite{HoferORM08}
describes embedding DSLs in Scala. In this paper we don't address internal DSLs,
because IDE support for the embedded languages is not available in these cases,
and we consider IDE support for the composed languages essential.
The landmark work of Hudak~\cite{1998_hudak_modular_dsl_and_tools} introduces
embedded DSLs as language extensions of Haskell. While Haskell provides advanced
concepts that enable such extensions, the new DSLs are essentially just
libraries built with the host language and are not first class language
entities: they do not define their own syntax, compiler errors are expressed in
terms of the host language, no custom semantic analyses are supported and no
specific IDE-support is provided. Essentially all internal DSLs expressed with
dynamic languages such as Ruby or Groovy, but also those embedded
in static languages such as Scala suffer from these limitations. 


