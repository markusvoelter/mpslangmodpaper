1) The author classifies language composition according two orthogonal
dimensions: the dependencies among languages (language
dependent/independent) and whether syntactic composition is supported
(fragment structure is homogenous/heterogeneous). In such way four
different language compositions is obtained (referencing, extensions,
reuse, embedding). They are at least two problems with this:

a) In section 4.5 the author introduces another kind of composition Ð
language annotations, which canÕt be classified using the proposed
classification. This is very odd and immediately diminishes importance
of such classification. Probably, author should be more explicit in
stating that language annotations is just a special case of language
semantic composition (if I understood properly) and that the proposed
classification is taking into account only syntactic composition. 
Of course, this is a shortcoming of the classification which needs to be
explicitly mentioned.
 
-> thank you for raising this issue. Annotations are actually a clear
   example of embedding: no dependencies, but syntactic mixing. In fact,
   it is an example of embedding that can work *without* an adapter
   language. I have clarified this in the introduction to Annotations.
   With this change, I think it strengthens the classification.

To me, the semantic composition is equally, if not more, important than 
syntactic composition. As usually, syntactic level is easier than semantic level.

-> I really don't agree. Defining the semantics for a DSL has always 
   been done by generating executable code in a lower-level language 
   (or by running an interpreter). While the level of "formality" for 
   this way of defining semantics may not be sufficient for everybody's
   needs, it certainly does work. However, as can be seen from the related
   work, syntactic aspects, especially the more fine-grained extension
   (on expression level, for example) or embedding (without escape char-
   acters) has not been easy in the past (unless you count specific,
   invasive changes to a base language). MPS solves this problem to a
   degree that has not been possible before.

   I do agree that MPS' main contribution is on syntactic level, exploiting
   the benefits of the projectional editor. I will emphasize this more in the 
   paper. TODO The other aspects mainly use established techniques or improve 
   only slightly on them.   

   TODO do more here?


a) Naming conventions Ð author picked the names (e.g., extension,
embedding, reuse) which have already established terminology in
software language engineering (SLE) and overload them with different
meaning. This is not a good practice and will further bring additional
confusion into SLE. A nice example is even contradiction with MPS
itself (see footnote on page 14: ÒMPS uses the term "extension"
whenever the definition of one language uses or refers to concepts
defined in another language. This is not necessarily an example of
language Extension as defined in this paper.Ó Not sure how to solved
this problem. But, if author want that his classification is going to
be accepted among SLE researchers these terms must be renamed.

-> Terms such as extension, embedding and reuse are used all over the place.
   It is very hard to come up with "unused" terms. In fact, I have spent
   a long time discussing these terms with others, and had also used other
   terms earlier. I don't think I can come up with something better.
   
   Also, I do define the terms very clearly in this paper, so a mis-
   understanding in the context of this paper is unlikely. I think that
   more unique terms (such as Piggybacking in the Mernik
   paper) are not much help either, since, in order for them to be under-
   stood by the reader, one has to refer to the paper and explain the term
   as well -- just from the word, it is not clear what it means. 

   Finally, the fact that MPS uses the term extends in another way is
   unfortunate, but, again, it is almost impossible to find terms that
   are not used by somebody in some other way. I only use the MPS term
   once in the paper, and through the footnote, I make clear the distinction
   in this specific place.

2) I donÕt agree with the statement on page 2: ÒEspecially syntactic
integration has traditionally been very hard [23] and hence is often
not supported for a particular combination of languages.Ó In fact,
semantic combination of languages and IDE combination are even harder.
If paper [23] is dealing only with syntactic composition this doesnÕt
mean that is the hardest part of language/IDE composition. Syntactic
language composition is only the first step. I donÕt see at which
level language composition can be easier than on syntactic level,
which comprises of lexical and syntactic language definitions.

-> See my comment above. 

   I have rewritten this paragraph to not downplay semantics as much
   as before. I have also emphasized IDE support in the context of
   syntactic composition.

3) Description of fragments (program is composed of several fragments
that may reference each other) and further explanation that fragment
in a file-based tools corresponds to files (page
4) is very confusing. Using this analogy I have hard times to see that
the fragment on page 14 is homogenous, 

-> ... because it only contains code from one language. I have added 
   a discussion that clarifies that adding a name-based reference to
   an element in a different fragment/language does not make the frag-
   ment heterogeneous, because the reference is just a identifier (a
   terminal). No reuse/embedding of non-terminals from the referenced
   language is required.

while the fragment on page 17 is heterogeneous. 

-> here we have embedded a new, non-trivial (i.e. non-terminal) language
   concept (block expression) into an otherwise unchanged language (Java).

Since the author didnÕt number fragments in the
paper I canÕt be more precise about which fragments I am referring to.

-> well, the one on page 17 is in a numbered figure... 

Looking to these fragments seems that analogy with files is not valid.

-> The first of the two examples is a complete fragment. There is also
   no reason why this could not be a file. I cannot see how the analogy 
   from fragments to files (in a file-based tool) is a problem.
   
   The second one is quite obviously a part of a fragment/file. I think
   it is accepted practice to only show parts of files in papers. Also, 
   I don't claim anywhere that the second one is a full fragment.

Another explanation for fragment is that is standalone AST. But, how a
standalone AST can be heterogeneous? 

-> Why not? If I have a file with code expressed in several languages,
   then the the overall AST is standalone (i.e. there is a node that
   has no parent) but still the nodes in the AST are instances of 
   concepts from different languages. I cannot see how this is a problem.

I think better explanation is needed also providing an analogy with 
non-terminals, which represent language concept (e.g., declaration, 
expressions, commands).

-> I do not understand this comment. 

4) The author wrote on page 5: ÒIn our experience, transformations are
the most widely used approach for defining semantics.Ó Although, this
might be indeed authorÕs experience it doesnÕt mean that this is the
best way to define the semantics. Readers might get an invalid
impression that this is the correct approach. See the excellent paper
on this issue: ÒMeaningful Modeling: What's the Semantics of
"Semantics"?Ó by David Harel and Bernhard Rumpe, IEEE Computer, Volume
37, Issue 10 (October 2004), Pages: 64 - 72.

-> I read the paper. The authors explain that semantics is to be defined  
   by a mapping of the syntactic constructs of a language L to a formalism
   F whose semantics is known - the semantic domain. They acknowledge that
   different degrees of formality can be used for F and the mapping L -> F.
   In that sense, transformation and code generation are valid means of
   defining semantics, although the authors of the reference above suggest
   mathematics as maybe the best appraoch. I will reference the paper, 
   but I don't think it invalidates fundamentally what I say in the paper.

5) The author wrote on page 5: ÒOther concerns are relevant in IDEs,
including refactoring, quick fixes, support for testing, debugging and
version control integration. While all of these are supported by MPS
in a modular and composable way, we do not discuss those aspects in
this paper to keep the paper at a reasonable length.Ó This should be
toned down since profiling is not supported in MPS (see page 33), or
debugging is too low-level (see page 33). The reader might get an
impression that everything is already solved in MPS.

-> agreed. I have changed it.

6) The author wrote on Page 7: Ò...also propose Piggybacking and
Pipelining as ways of reusing existing generators or interpreters. We
do not include these approaches in our discussion here because they do
not compose languages - they just chain their transformations.Ó While
this is true for pipelining, it is very wrong for piggybacking! Using
piggyback approach a language L1 reuse ('reuse' is not necessary as
defined in this paper) some parts of another language L2 (e.g., only
Java assignment command). The language L1 is a combination of some
concepts from the language L2 and other concepts from the language L1.

-> ok, my mistake. I have changed this.  


7) All examples (fragments) must be numbered. Simply referring to
fragments as Òbelow is an exampleÓ is odd when the example actually
appears in next page (see pages 20-21, 25-26).

-> I strongly disagree. Having all these numbers in the text doesn't make 
   them easier to find, in particular, if they are moved to other pages.
   I have taken great care to put the code examples close to the place where
   they are referenced. In very few places are they on the next page. I think
   it is acceptable to expect the user, to flip to the next page if he reads
   "below" and there's no code "below" on the same page. 

8) I donÕt see how the following discussion on page 33 is relevant for
MPS debuggers: ÒHowever, as part of the mbeddr project [49] that
develops an extensible version of the C programming language we have
developed a framework for extensible C debuggers. Developers of C
extensions can easily specify how the extension integrates into the C
debugger so that debugging on the syntax of the extension becomes
possible for heterogeneous fragments. Visser et al. also describe an
approach for DSL debugging in [26].Ó

-> I have clarified the connection to MPS: it is likely that the 
   extensible debugger framework underlying mbeddr will be moved
   into MPS.

Moreover, DSL debuggers and test engines have been discussed also in:

WU, Hui, GRAY, Jeffrey G., MERNIK, Marjan. Grammar-driven generation
of domain-specific language debuggers. Softw. pract. exp., 2008, vol.
38, iss. 10, pp. 1073-1103

WU, Hui, GRAY, Jeffrey G., MERNIK, Marjan. Unit testing for
domain-specific languages. Lect. notes comput. sci., 2009, vol. 5658,
pp. 125-147

-> I have added a reference to the first of the two.

9) The author wrote on page 40 regarding the LISA tool: ÒCombination
of independently developed grammars (or sub-grammars) is not
supported.Ó
This claim is wrong! LISA supports such a combination using multiple
attribute grammar inheritance. Below is a small example how
independent grammars (and semantics) can be combined.

-> Thank you for the example. I had the feeling that it should be
   possible but hadn't found any example in the literature I found
   on LISA. I have removed the respective statement.


10) Discussion of homogenous approach in Helvetia and heterogeneous
approach in MPS (section 6) is not the best since terms ÔhomogenousÕ
and ÔheterogeneousÕ have been heavily used also for fragments in this
paper. The reader will be confused.

-> good point. I have removed the two terms from this discussion.
   I have kept the discussion in place, though.

11) The name for Cdn (page 4) is not explained. What Cdn stands for?

-> it stands for Children. I hoped this would be clear from the context.
   I have added an explanation (also for Refs and Inh).

12) When domain-specific languages (DSLs) are referred for the first
time (Introduction, page 2) the author should provide references and
classification into internal and external DSLs. This classification is
used in Section 6 without proper introduction in this paper.



13) Overloaded terms come again to play when the author start
discussion on DSL embedding. Will average reader be able to
differentiate embedding presented in this paper with embedding in work
of Hudak and Hofer? I donÕt think so.

14) The author should compare their classification with the
classification on language composition presented in the paper:

S. Erdweg, P. G. Giarrusso, T. Rendel. Language Composition Untangled.
Proceedings of Workshop on Language Descriptions, Tools and
Applications (LDTA'12), 2012.


15) Some problems in references still exist. In particular,
a) 21. G. L. S. Jr. Growing a Language
// Last name ?

-> yes, unfortuntely Latex was confused by the Jr. after Steele. Fixed.

b) 29. Martin Bravenboer and Eelco Dolstra and Eelco Visser
// Full first names appear only in this reference

-> fixed.

- Page 30: Concpet -> Concept

-> fixed.