\section{Implementation Challenges and Solutions}
\label{challenges}

The previous section has introduced four \lmrc{} approaches. In this
section we investigate the consequences of these approaches for the composition
of concrete syntax, type system and transformation definitions.

\subsection{Syntax}

\paragraph{Referencing and Reuse} keeps fragments homogeneous. Mixing of
concrete syntax is not required. A reference between fragments is usually simply
an identifier and does not have its own internal structure for which a grammar
would be required. The name resolution phase can then create the actual
cross-reference between abstract syntax objects. In the refrigerator example,
the algorithm language contains cross-references into the hardware language.
Those references are simple, dotted names ($a.b.c$). In the UI example, the
adapter language simply introduces dotted names to refer to fields of data
structures.

\paragraph{Extension and Embedding} requires modular concrete syntax definitions
because additional language elements must be "mixed" with programs written with
the base language. In the embedded programming example, state machines are
hosted in regular C programs. This works because the C language's
\verb#Module# construct contains a collection of \verb#ModuleContents#, and the
\verb#StateMachine# concept extends the \verb#ModuleContent# concept. This state
machine language is designed specifically for being embedded into C, so it can
access and extend the \verb#ModuleContent# concept. If the state machine
language were reusable with any host language in addition to C, then an adapter
language would provide a language concept that adapts C's \verb#IModuleContent#
to \verb#StateMachine#, because \verb#StateMachine# cannot directly extend 
\verb#IModuleContent# --- it does now depend on the C language.


\subsection{Type Systems}

\paragraph{Referencing} The type system rules and constraints of the referencing
language typically have to take into account the referenced language. Since the
referenced language is known when developing the referencing language, the type
system can be implemented with the referenced language in mind as well. In the
refrigerator example, the algorithm language defines typing rules for hardware
elements (from the hardware language), because these types are used to determine
which properties can be accessed on the hardware elements. For example, a
compressor has a property \verb#active# that controls if it is turned on or off.

\paragraph{Extension} The type systems of the base language must be designed in
a way that allows adding new typing rules in language extensions. For example,
if the base language defines typing rules for binary operators, and the
extension language defines new types, then those typing rules may have to be
overridden to allow the use of existing operators with the new types. In the
embedded systems example, a language extension provides types with physical
units (as in 100 kg). Additional typing rules are needed to override the typing
rules for C's basic operators (+, -, *, /, etc.).

\paragraph{Reuse and Embedding} The typing rules that affect the interplay
between the two languages reside in the adapter language. In the UI
example the adapter language will have to adapt the data types of the fields in
the data description to the types the UI widgets expect. For example, a combo
box widget can only be bound to fields that have some kind of text or enum data
type. Since the specific types are specific to the data description language
(which is unknown at the time of creation of the UI language), a mapping must be
provided in the adapter language. 


\subsection{Transformation}

In this section we use the terms \emph{transformation} and \emph{generation}
interchangably. In general, the term transformation is used if one tree of
program elements is mapped to another tree, while generation describes the case
of creating text from program trees. However, for the discussions in this section,
this distinction is generally not relevant. 

\paragraph{Referencing} Three cases have to be considered. The first one
(\fig{transReferencing} A) propagates the referencing structure to two
independent target fragments. We call these two transformations single-sourced,
since each of them only uses a single, homogeneous fragment as input and creates
a single, homogeneous fragment, perhaps with references between them. Since the
referencing language is created with the knowledge about the referenced
language, the generator for the referencing language can be written with
knowledge about the names of the elements that have to be referenced in the
other fragment. If a generator for the referenced language already exists, it
can be reused unchanged. The two generators basically share naming information.

The refrigerator example uses this case. From the hardware description we
generate an XML file that configures a framework that actually collects the data
behind the properties of the hardware components in the running refrigerator.
The C code generated from the algorithm accesses that framework, using
agreed-upon identifiers to identify properties whose value have to be read or
set.

\begin{figure}[h]  
\begin{center}
  \includegraphics[width=8.2cm]{figures/transReferencing.png}
  \caption[labelInTOC]{Three alternatives of handling the transformations for
  language referencing: (A) two separate, dependent, single-source
  transformations, (B) a single multi-sourced transformation and (C) a
  preprocessing transformation that changes the referenced fragment in a way
  specified by the referencing fragment, then reusing existing transformations
  for the referenced fragment (Notation for this and the following
  illustrations: boxes are fragments, fat arrows are transformations, thin
  arrows are dependencies and shading represents languages.)}
  \label{transReferencing}  
\end{center} 
\end{figure} 

The second case (\fig{transReferencing} B) is a multi-sourced transformation
that creates one single homogeneous fragment. This typically occurs if the
referencing fragment is used to guide the transformation of the referenced
fragment, for example by specifying target transformation strategies. In this
case, a new transformation has to be written that takes the referencing fragment
into account. The possibly existing generator for the referenced language cannot
be reused as is. An alternative to rewriting the generator is the use of a
preprocessing transformation (\fig{transReferencing} C), that changes the
referenced fragment in a way consistent with what the referenced fragment
prescribes. The existing transformations for the referenced fragment can then be
reused.
 
\begin{figure}[h]
\begin{center}
  \includegraphics[width=6cm]{figures/transExtension.png}
  \caption[labelInTOC]{Transformation for language extensions usually happens
  by assimiliation, i.e. generating code in the host language from code
  expressed in the extension language. Optionally, additional files are
  generated, often some configuration files.}
  \label{transExtension} 
\end{center}
\end{figure} 

\paragraph{Extension} As we have discussed above, language extensions are
usually created by defining linguistic abstractions for common idioms of a
domain $D$. A generator for the new language concepts can simply recreate those
idioms when mapping $L_D$ to $L_{D-1}$, a process called assimilation. In other
words, transformations for language extensions map a heterogeneous fragment
(containing $L_{D-1}$ and $L_D$ code) to a homogeneous fragment that contains
only $L_{D-1}$ code (\fig{transExtension}). In some cases additional files may
be generated, often configuration files. In the embedded systems state machines
example, the state machines are generated down to a function that contains a
switch/case statement, as well as enums for states and events. 

Sometimes a language extension requires rewriting transformations defined by the
base language. For example, in the data-types-with-physical-units example, the
language also provides range checking and overflow detection. So if two such
quantities are added, the addition is transformed into a call to a special
\verb#add# function instead of using the regular plus operator. This function
performs overflow checking and addition.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=8.2cm]{figures/transReuse.png} 
  \caption[labelInTOC]{Transformation in the case of reuse comes in three
  flavors. reuse of existing transformations for both fragments plus
  generation of adapter code (A), composing transformations (B) or generating
  separate artifacts plus a weaving specification (C).}
  \label{transReuse}  
\end{center} 
\end{figure}

Language Extension introduces the risk of semantic interactions. The
transformations associated with several independently developed extensions of
the same base language may interact with each other. Consider the (somewhat
constructed) example of two extensions to Java that each define a new statement.
When assimilated to pure Java, both new statements require the surrounding Java
class to extend a specific, but different base class. This won't work because a
Java class can only extend one base class. Interactions may also be more subtle
and affect memory usage or execution performance. Note that this problem is not
specific to languages, it can occur whenever several independent extensions of a
base concept can be used together, ad hoc. To avoid the problem, transformations
should be built in a way so that they do not "consume scarce resources" such as
inheritance links. A more thorough discussion of the problem of
semantic interactions is beyond the scope of this paper.

\paragraph{Reuse} In the reuse scenario, it is likely that both the reused and
the context language already come with their own generators. If these generators
transform to different, incompatible target languages, no reuse is possible. If
they transform to a common target languages (such as Java or C) then the
potential for reusing previously existing transformations exists.



There are three cases to consider. The first one, illustrated in
\fig{transReuse} A, describes the case where there is an existing transformation
for the reused fragment and an existing transformation for the context fragment
--- the latter being written with the knowledge that later extension will be
necessary. In this case, the generator for the adapter language may "fill in the
holes" left by the reusable generator for the referencing language For example,
the generator of the context language may generate a class with abstract
methods; the adapter may generate a subclass and implement these abstract
methods. In the second case, \fig{transReuse} B, the existing generator for the
reused fragment has to be enhanced with transformation code specific to the
context language. In this case, a mechanism for composing transformations is
needed. The third case, \fig{transReuse} C, leaves composition to the target
languages. We generate three different independent, homogeneous fragments, and
some kind of weaver composes them into one final, heterogeneous artifact. Often,
the weaving specification is the intermediate result generated from the adapter
language. An example implementation could use AspectJ.

\begin{figure}[h]
\begin{center} 
  \includegraphics[width=8.2cm]{figures/transEmbedding.png} 
  \caption[labelInTOC]{In transforming embedded languages, a new transformation
  has to be written if the embedded language does not come with a
  transformation for the target language of the host language transformation.
  Otherwise the adapter language can coordinate the transformations for the
  host and for the emebedded languages.}
  \label{transEmbedding}  
\end{center} 
\end{figure} 



\paragraph{Embedding} An embeddable language may not come with its own
generator, since, at the time of implementing the embeddable language, one
cannot know what to generate. In that case, when embedding the language, a
suitable generator has to be developed. It will typically either generate host
language code (similar to generators in the case of language extension) or
directly generate to the same target language that is generated to by the host
language.

If the embeddable language comes with a generator that transforms to the same
target language as the embedding language, then the generator for the
adapter language can coordinate the two, and make sure a single, consistent
fragment is generated. \fig{transEmbedding} illustrates this case.

Just a language extension, language embedding may also lead to semantic
interactions if multiple languages are embedded into the same host language. 



\section{Tool Support}
\label{tools}

In this section we look discuss to what extent our the example tools
introduced in \sect{toolintro} support the \lmrc\ approaches and the
implementation challenges.  

\subsection{Eclipse Xtext}

\paragraph{Syntax} Language referencing is supported by Xtext. The grammar
language comes with special support to reference concepts defined in other
languages (or Ecore-based meta model in general). Language extension is also
supported, but for at most one base language. Consequently, language reuse is
possible as well. However language embedding is not supported because of the
limitation to extend only one other language.

% This limitation seriously limits language
% modularization and composition with Xtext.

\paragraph{Constraints/Type Systems} Constraints are implemented in Xtext as
Java methods that query the AST and report errors. Xtext does not directly
support a concise definition of type systems, but third party solutions exist,
e.g. the Xtext Typesystem Framework\footnote{http://code.google.com/a/eclipselabs.org/p/xtext-typesystem/}.
Xtext also comes with XBase, a reusable expression language. Languages can
extend Xbase to reuse these expressions. Xbase comes with a framework for type
calculations. This framework can be extended in a principled way to integrate
the typing rules of a language that extends XBase.

\paragraph{Transformation} Xtext comes with a transformation and code generation
language called Xtend. It supports dependency injection as a first class
citizen. This makes it possible to override parts of generators selectively, if
the original generator developer has delegated the relevant parts to injected
classes. This facility supports the transformation challenges outlined
in the previous section nicely.

\subsection{JetBrains MPS}

\paragraph{Syntax} MPS supports syntax composition. Since no grammar is used,
the extending language simply creates new language constructs, some of them
extending concepts from the base language. In cases where ambiguity would arise
in a grammar-based system, MPS requires the language user to decide during input
which of the alternatives should be used. A language in MPS can extend any
number of languages. In particular, for a given base language, a program can be
written using any number of languages extending this base language, without
first defining a composed language explicitly. For example, a set of extensions
to C can be developed independently, and users can then decide ad hoc which
extensions to use in a program (as long as the extensions are semantically
compatible). From a syntactic perspective, all \lmrc\ approaches are supported
by MPS without any limits.

MPS also supports restriction. A language concept can restrict under which
parents it can be used, or which children a concept may have. These constraints
literally remove the ability to enter constructs that would violate the
constraints. Using this approach, the unit testing extension to C enforces that
\verb#assert# statements can only be used in unit tests and not in any other
statement list.

MPS has another approach for language embedding called \emph{attributes}. An
attribute introduces additional parent-child relationships into concepts defined
in other languages, without invasively changing the definition of that other
language. By making an attribute apply to \texttt{BaseConcept}, the super
concept of all concepts (comparable to \texttt{java.lang. Object}), attributes
can be made applicable to arbitrary program elements, effectively supporting AOP-like
introduction on the language level. Attributes are typically used for
adding "meta data" such as documentation, presence conditions in product line
engineering \cite{Voelter2010} or requirements traces. 

\paragraph{Constraint/Type Systems} MPS comes with a DSL for expressing type
system rules. It is based on a unification engine and supports type inference.
For typing rules that should be extensible, MPS supports
so-called overloaded operation containers. This mechanism supports plugging in
additional typing rules by adding them to the language definition. For
example, the typing rules for binary operators in C are implemented this
way. The language extension that adds types with physical units provides
additional typing rules for the case when the left and/or right argument of a
binary operator is a type with a physical unit.

\paragraph{Transformation} MPS distinguishes two cases. The first one covers
text generation. Text generators are relatively inflexible regarding
modularization, but they are used only for the lowest level languages (typically
GPLs covering $D_0$ such as Java or C). All other "generations" happens by
transformation rules that work on the projected tree, typically using
assimilation. For example, the concepts in a state machine extension to C are
transformed back to the concepts in the C language it extends. Overwriting
existing generators is also possible by modularizing generators in a suitable
way and then using generator priorities to make the overriding generator run
before the overridden one. It is also possible to define hooks in generators by
generating output elements whose only purpose is to be transformed further by
subsequent generators. By plugging in different subsequent generators, the same
hook can be transformed into different end results.

Note that MPS does not provide support for systematically handling semantic
interactions. Transformations will just fail in case an unintended interaction
occurs. To avoid this, transformations have to be designed carefully. Note that
a simple approach to lessening the problem would be to be able to declare a
language as being \emph{incompatible} with another one. In that case an error
could be reported as soon as the languages are used together in a fragment.
  

\subsection{Spoofax} 

\paragraph{Syntax}

Syntax definition in Spoofax is based on the SDF2 Syntax Definition Formalism
\cite{Vis97.thesis}. Since SDF uses the Scannerless Generalized-LR parsing
algorithm, the full class of context-free grammars is supported, which is the
only grammar class closed under composition, unlike the restricted grammar
classes of other parser generators. Furthermore, the parser does not use a
separate lexical analysis phase, but rather considers individual tokens as
characters, which entails that it supports combination of languages with a
different lexical syntax, as demonstrated, for example, in the syntax definition
for AspectJ~\cite{BravenboerTV06}. Thus, SDF supports both extension and
embedding of languages.

\paragraph{Analysis and Transformation}

Semantic analysis, transformation, and code generation in Spoofax are defined
with the Stratego transformation language \cite{VisserBT98,BravenboerKVV08},
which is based on the paradigm of term rewriting with programmable strategies.
Basic transformations are defined with rewrite rules and can be combined into
composite transformations using strategies. Thus, basic rules defining type
analysis, static semantic constraints, desugarings, and other transformations
for separate language components can be composed into transformations for
language compositions. Stratego provides aspects for overriding and adapting
transformation strategy definitions~\cite{KatsVisser-SCAM-2010}. 




